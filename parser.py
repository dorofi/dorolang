from abc import ABC, abstractmethod
from typing import List, Optional, Union
from dataclasses import dataclass

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à –ª–µ–∫—Å–µ—Ä
from enum import Enum, auto
import re

# –ö–æ–ø–∏—Ä—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑ –ª–µ–∫—Å–µ—Ä–∞ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ —ç—Ç–æ –±—ã–ª–æ –±—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö)
class TokenType(Enum):
    NUMBER = auto()
    STRING = auto()
    IDENTIFIER = auto()
    PRINT = auto()
    LET = auto()
    ASSIGN = auto()
    PLUS = auto()
    MINUS = auto()
    MULTIPLY = auto()
    DIVIDE = auto()
    NEWLINE = auto()
    EOF = auto()
    UNKNOWN = auto()

@dataclass
class Token:
    type: TokenType
    value: str
    line: int
    column: int

# ============== AST NODE DEFINITIONS ==============

class ASTNode(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤ AST"""
    pass

class Expression(ASTNode):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
    pass

class Statement(ASTNode):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
    pass

# EXPRESSIONS (–≤—ã—Ä–∞–∂–µ–Ω–∏—è - —Ç–æ —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ)

@dataclass
class NumberLiteral(Expression):
    """–ß–∏—Å–ª–æ–≤–æ–π –ª–∏—Ç–µ—Ä–∞–ª: 42, 3.14"""
    value: Union[int, float]
    
    def __str__(self):
        return f"Number({self.value})"

@dataclass
class StringLiteral(Expression):
    """–°—Ç—Ä–æ–∫–æ–≤—ã–π –ª–∏—Ç–µ—Ä–∞–ª: "hello", 'world'"""
    value: str
    
    def __str__(self):
        return f"String({self.value})"

@dataclass
class Identifier(Expression):
    """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π: name, age"""
    name: str
    
    def __str__(self):
        return f"Var({self.name})"

@dataclass
class BinaryOperation(Expression):
    """–ë–∏–Ω–∞—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: a + b, x * y"""
    left: Expression
    operator: str
    right: Expression
    
    def __str__(self):
        return f"({self.left} {self.operator} {self.right})"

# STATEMENTS (—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è - —Ç–æ —á—Ç–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è)

@dataclass
class PrintStatement(Statement):
    """Print —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: print expression"""
    expression: Expression
    
    def __str__(self):
        return f"Print({self.expression})"

@dataclass
class AssignmentStatement(Statement):
    """–ü—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ: let name = expression"""
    identifier: str
    expression: Expression
    
    def __str__(self):
        return f"Assign({self.identifier} = {self.expression})"

@dataclass
class Program(ASTNode):
    """–ö–æ—Ä–µ–Ω—å –ø—Ä–æ–≥—Ä–∞–º–º—ã - —Å–ø–∏—Å–æ–∫ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
    statements: List[Statement]
    
    def __str__(self):
        statements_str = "\n  ".join(str(stmt) for stmt in self.statements)
        return f"Program:\n  {statements_str}"

# ============== PARSER ==============

class ParseError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    def __init__(self, message: str, token: Token):
        self.message = message
        self.token = token
        super().__init__(f"{message} at line {token.line}, column {token.column}")

class Parser:
    def __init__(self, tokens: List[Token]):
        self.tokens = tokens
        self.position = 0
    
    def current_token(self) -> Token:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω"""
        if self.position >= len(self.tokens):
            return self.tokens[-1]  # EOF —Ç–æ–∫–µ–Ω
        return self.tokens[self.position]
    
    def peek_token(self, offset: int = 1) -> Token:
        """–°–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Ç–æ–∫–µ–Ω –≤–ø–µ—Ä–µ–¥–∏"""
        peek_pos = self.position + offset
        if peek_pos >= len(self.tokens):
            return self.tokens[-1]  # EOF —Ç–æ–∫–µ–Ω
        return self.tokens[peek_pos]
    
    def advance(self) -> Token:
        """–ü–µ—Ä–µ–º–µ—â–∞–µ—Ç—Å—è –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Ç–æ–∫–µ–Ω—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–π"""
        current = self.current_token()
        if self.position < len(self.tokens) - 1:
            self.position += 1
        return current
    
    def match(self, expected_type: TokenType) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ç–∏–ø—É"""
        return self.current_token().type == expected_type
    
    def consume(self, expected_type: TokenType, error_message: str = "") -> Token:
        """–ü–æ—Ç—Ä–µ–±–ª—è–µ—Ç —Ç–æ–∫–µ–Ω –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Ç–∏–ø–∞ –∏–ª–∏ –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É"""
        if self.match(expected_type):
            return self.advance()
        
        if not error_message:
            error_message = f"Expected {expected_type.name}, got {self.current_token().type.name}"
        
        raise ParseError(error_message, self.current_token())
    
    def skip_newlines(self) -> None:
        """–ü—Ä–æ–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫"""
        while self.match(TokenType.NEWLINE):
            self.advance()
    
    def parse(self) -> Program:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –ø–∞—Ä—Å–∏—Ç –≤—Å—é –ø—Ä–æ–≥—Ä–∞–º–º—É"""
        statements = []
        
        self.skip_newlines()  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –Ω–∞—á–∞–ª–µ
        
        while not self.match(TokenType.EOF):
            if self.match(TokenType.NEWLINE):
                self.advance()
                continue
            
            try:
                stmt = self.parse_statement()
                statements.append(stmt)
            except ParseError as e:
                print(f"Parse error: {e}")
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
                while not self.match(TokenType.NEWLINE) and not self.match(TokenType.EOF):
                    self.advance()
        
        return Program(statements)
    
    def parse_statement(self) -> Statement:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        # Print statement
        if self.match(TokenType.PRINT):
            return self.parse_print_statement()
        
        # Assignment statement (let x = ...)
        elif self.match(TokenType.LET):
            return self.parse_assignment_statement()
        
        else:
            raise ParseError(f"Unexpected token: {self.current_token().value}", self.current_token())
    
    def parse_print_statement(self) -> PrintStatement:
        """–ü–∞—Ä—Å–∏—Ç print —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        self.consume(TokenType.PRINT)  # –ü–æ—Ç—Ä–µ–±–ª—è–µ–º 'print'
        expression = self.parse_expression()  # –ü–∞—Ä—Å–∏–º –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –ø–µ—á–∞—Ç–∏
        return PrintStatement(expression)
    
    def parse_assignment_statement(self) -> AssignmentStatement:
        """–ü–∞—Ä—Å–∏—Ç –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏–µ: let name = expression"""
        self.consume(TokenType.LET)  # –ü–æ—Ç—Ä–µ–±–ª—è–µ–º 'let'
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        identifier_token = self.consume(TokenType.IDENTIFIER, "Expected variable name after 'let'")
        identifier_name = identifier_token.value
        
        # –ü–æ—Ç—Ä–µ–±–ª—è–µ–º –∑–Ω–∞–∫ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–Ω–∏—è
        self.consume(TokenType.ASSIGN, "Expected '=' after variable name")
        
        # –ü–∞—Ä—Å–∏–º –≤—ã—Ä–∞–∂–µ–Ω–∏–µ —Å–ø—Ä–∞–≤–∞ –æ—Ç =
        expression = self.parse_expression()
        
        return AssignmentStatement(identifier_name, expression)
    
    def parse_expression(self) -> Expression:
        """–ü–∞—Ä—Å–∏—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏–µ (—Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤)"""
        return self.parse_addition()
    
    def parse_addition(self) -> Expression:
        """–ü–∞—Ä—Å–∏—Ç —Å–ª–æ–∂–µ–Ω–∏–µ –∏ –≤—ã—á–∏—Ç–∞–Ω–∏–µ (–Ω–∏–∑—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"""
        left = self.parse_multiplication()
        
        while self.match(TokenType.PLUS) or self.match(TokenType.MINUS):
            operator = self.advance().value
            right = self.parse_multiplication()
            left = BinaryOperation(left, operator, right)
        
        return left
    
    def parse_multiplication(self) -> Expression:
        """–ü–∞—Ä—Å–∏—Ç —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ –¥–µ–ª–µ–Ω–∏–µ (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"""
        left = self.parse_primary()
        
        while self.match(TokenType.MULTIPLY) or self.match(TokenType.DIVIDE):
            operator = self.advance().value
            right = self.parse_primary()
            left = BinaryOperation(left, operator, right)
        
        return left
    
    def parse_primary(self) -> Expression:
        """–ü–∞—Ä—Å–∏—Ç –ø–µ—Ä–≤–∏—á–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (—á–∏—Å–ª–∞, —Å—Ç—Ä–æ–∫–∏, –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)"""
        # –ß–∏—Å–ª–∞
        if self.match(TokenType.NUMBER):
            token = self.advance()
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º int –∏–ª–∏ float
            if '.' in token.value:
                return NumberLiteral(float(token.value))
            else:
                return NumberLiteral(int(token.value))
        
        # –°—Ç—Ä–æ–∫–∏
        elif self.match(TokenType.STRING):
            token = self.advance()
            # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏
            string_value = token.value[1:-1]  # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–∏–º–≤–æ–ª (–∫–∞–≤—ã—á–∫–∏)
            return StringLiteral(string_value)
        
        # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã (–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ)
        elif self.match(TokenType.IDENTIFIER):
            token = self.advance()
            return Identifier(token.value)
        
        else:
            raise ParseError(f"Unexpected token in expression: {self.current_token().value}", 
                           self.current_token())
    
    def pretty_print_ast(self, node: ASTNode, indent: int = 0) -> None:
        """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç AST –¥–µ—Ä–µ–≤–æ"""
        prefix = "  " * indent
        
        if isinstance(node, Program):
            print(f"{prefix}Program:")
            for stmt in node.statements:
                self.pretty_print_ast(stmt, indent + 1)
        
        elif isinstance(node, PrintStatement):
            print(f"{prefix}PrintStatement:")
            self.pretty_print_ast(node.expression, indent + 1)
        
        elif isinstance(node, AssignmentStatement):
            print(f"{prefix}Assignment: {node.identifier} =")
            self.pretty_print_ast(node.expression, indent + 1)
        
        elif isinstance(node, BinaryOperation):
            print(f"{prefix}BinaryOp: {node.operator}")
            self.pretty_print_ast(node.left, indent + 1)
            self.pretty_print_ast(node.right, indent + 1)
        
        elif isinstance(node, NumberLiteral):
            print(f"{prefix}Number: {node.value}")
        
        elif isinstance(node, StringLiteral):
            print(f"{prefix}String: \"{node.value}\"")
        
        elif isinstance(node, Identifier):
            print(f"{prefix}Variable: {node.name}")


# ============== –ü–†–û–°–¢–û–ô –õ–ï–ö–°–ï–† –î–õ–Ø –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø ==============

class SimpleLexer:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ª–µ–∫—Å–µ—Ä–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞"""
    
    def __init__(self, source_code: str):
        self.source = source_code
        self.position = 0
        self.line = 1
        self.column = 1
        
        self.token_patterns = [
            (r'print', TokenType.PRINT),
            (r'let', TokenType.LET),
            (r'\d+\.?\d*', TokenType.NUMBER),
            (r'"[^"]*"', TokenType.STRING),
            (r"'[^']*'", TokenType.STRING),
            (r'[a-zA-Z_][a-zA-Z0-9_]*', TokenType.IDENTIFIER),
            (r'=', TokenType.ASSIGN),
            (r'\+', TokenType.PLUS),
            (r'-', TokenType.MINUS),
            (r'\*', TokenType.MULTIPLY),
            (r'/', TokenType.DIVIDE),
            (r'\n', TokenType.NEWLINE),
        ]
    
    def tokenize(self) -> List[Token]:
        tokens = []
        lines = self.source.split('\n')
        
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            pos = 0
            while pos < len(line):
                if line[pos] == ' ':
                    pos += 1
                    continue
                
                found_match = False
                for pattern, token_type in self.token_patterns:
                    match = re.match(pattern, line[pos:])
                    if match:
                        value = match.group(0)
                        tokens.append(Token(token_type, value, line_num, pos + 1))
                        pos += len(value)
                        found_match = True
                        break
                
                if not found_match:
                    pos += 1
        
        tokens.append(Token(TokenType.EOF, '', len(lines), 1))
        return tokens


# ============== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ==============

if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –∫–æ–¥ –Ω–∞ DoroLang
    test_code = '''
print "Hello, DoroLang!"
print 42
let name = "Dorofii"
print name
let age = 25
print age + 5
print "Age is " + age
let result = 10 * 2 + 3
print result
'''
    
    print("=== –ò–°–•–û–î–ù–´–ô –ö–û–î ===")
    print(test_code)
    print("\n" + "="*50)
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    lexer = SimpleLexer(test_code)
    tokens = lexer.tokenize()
    
    print("\n=== –¢–û–ö–ï–ù–´ ===")
    for i, token in enumerate(tokens):
        print(f"{i:2}: {token.type.name:12} '{token.value}'")
    
    print("\n" + "="*50)
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    parser = Parser(tokens)
    try:
        ast = parser.parse()
        
        print("\n=== ABSTRACT SYNTAX TREE ===")
        parser.pretty_print_ast(ast)
        
        print(f"\n=== –°–í–û–î–ö–ê ===")
        print(f"‚úÖ –¢–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
        print(f"‚úÖ –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π: {len(ast.statements)}")
        print("‚úÖ –ü–∞—Ä—Å–µ—Ä –≥–æ—Ç–æ–≤! üéâ")
        
    except ParseError as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")