import re
from enum import Enum, auto
from dataclasses import dataclass
from typing import List, Optional

class TokenType(Enum):
    # Literals
    NUMBER = auto()
    STRING = auto()
    IDENTIFIER = auto()
    
    # Keywords
    PRINT = auto()
    LET = auto()    # –¥–ª—è –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: let x = 5
    
    # Operators
    ASSIGN = auto()     # =
    PLUS = auto()       # +
    MINUS = auto()      # -
    MULTIPLY = auto()   # *
    DIVIDE = auto()     # /
    
    # Delimiters
    NEWLINE = auto()
    EOF = auto()        # End of file
    
    # Special
    UNKNOWN = auto()

@dataclass
class Token:
    type: TokenType
    value: str
    line: int
    column: int

class Lexer:
    def __init__(self, source_code: str):
        self.source = source_code
        self.position = 0
        self.line = 1
        self.column = 1
        self.tokens: List[Token] = []
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤
        self.token_patterns = [
            (r'print', TokenType.PRINT),
            (r'let', TokenType.LET),
            (r'\d+\.?\d*', TokenType.NUMBER),          # 123, 12.34
            (r'"[^"]*"', TokenType.STRING),            # "hello world"
            (r"'[^']*'", TokenType.STRING),            # 'hello world'
            (r'[a-zA-Z_][a-zA-Z0-9_]*', TokenType.IDENTIFIER),  # –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
            (r'=', TokenType.ASSIGN),
            (r'\+', TokenType.PLUS),
            (r'-', TokenType.MINUS),
            (r'\*', TokenType.MULTIPLY),
            (r'/', TokenType.DIVIDE),
            (r'\n', TokenType.NEWLINE),
        ]
    
    def current_char(self) -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å–∏–º–≤–æ–ª –∏–ª–∏ None –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –∫–æ–Ω—Ü–∞"""
        if self.position >= len(self.source):
            return None
        return self.source[self.position]
    
    def peek_char(self, offset: int = 1) -> Optional[str]:
        """–°–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Å–∏–º–≤–æ–ª –≤–ø–µ—Ä–µ–¥–∏ –±–µ–∑ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏"""
        peek_pos = self.position + offset
        if peek_pos >= len(self.source):
            return None
        return self.source[peek_pos]
    
    def advance(self) -> None:
        """–ü–µ—Ä–µ–º–µ—â–∞–µ–º—Å—è –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —Å–∏–º–≤–æ–ª—É"""
        if self.position < len(self.source):
            if self.source[self.position] == '\n':
                self.line += 1
                self.column = 1
            else:
                self.column += 1
            self.position += 1
    
    def skip_whitespace(self) -> None:
        """–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ —Ç–∞–±—ã (–Ω–æ –Ω–µ –Ω–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏)"""
        while (self.current_char() is not None and 
               self.current_char() in ' \t\r'):
            self.advance()
    
    def skip_comment(self) -> None:
        """–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å #"""
        if self.current_char() == '#':
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–æ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏
            while (self.current_char() is not None and 
                   self.current_char() != '\n'):
                self.advance()
    
    def read_string(self, quote_char: str) -> str:
        """–ß–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤ –∫–∞–≤—ã—á–∫–∞—Ö"""
        result = quote_char  # –í–∫–ª—é—á–∞–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∫–∞–≤—ã—á–∫—É
        self.advance()  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é –∫–∞–≤—ã—á–∫—É
        
        while (self.current_char() is not None and 
               self.current_char() != quote_char):
            if self.current_char() == '\\':
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                result += self.current_char()
                self.advance()
                if self.current_char() is not None:
                    result += self.current_char()
                    self.advance()
            else:
                result += self.current_char()
                self.advance()
        
        if self.current_char() == quote_char:
            result += self.current_char()  # –í–∫–ª—é—á–∞–µ–º –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é –∫–∞–≤—ã—á–∫—É
            self.advance()
        
        return result
    
    def tokenize(self) -> List[Token]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
        while self.position < len(self.source):
            self.skip_whitespace()
            
            # –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
            if self.current_char() is None:
                break
            
            # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
            if self.current_char() == '#':
                self.skip_comment()
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é –¥–ª—è —Ç–æ–∫–µ–Ω–∞
            token_line = self.line
            token_column = self.column
            
            # –°—Ç—Ä–æ–∫–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
            if self.current_char() in '"\'':
                quote_char = self.current_char()
                string_value = self.read_string(quote_char)
                self.tokens.append(Token(TokenType.STRING, string_value, token_line, token_column))
                continue
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º–∏
            found_match = False
            remaining_source = self.source[self.position:]
            
            for pattern, token_type in self.token_patterns:
                match = re.match(pattern, remaining_source)
                if match:
                    token_value = match.group(0)
                    self.tokens.append(Token(token_type, token_value, token_line, token_column))
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ø–æ–∑–∏—Ü–∏—é
                    for _ in range(len(token_value)):
                        self.advance()
                    
                    found_match = True
                    break
            
            if not found_match:
                # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª
                unknown_char = self.current_char()
                self.tokens.append(Token(TokenType.UNKNOWN, unknown_char, token_line, token_column))
                self.advance()
        
        # –î–æ–±–∞–≤–ª—è–µ–º EOF —Ç–æ–∫–µ–Ω
        self.tokens.append(Token(TokenType.EOF, '', self.line, self.column))
        return self.tokens
    
    def print_tokens(self) -> None:
        """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏–º –≤—Å–µ —Ç–æ–∫–µ–Ω—ã"""
        print("=== –¢–û–ö–ï–ù–´ ===")
        for i, token in enumerate(self.tokens):
            print(f"{i:2}: {token.type.name:12} '{token.value}' at {token.line}:{token.column}")


# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞—à –ª–µ–∫—Å–µ—Ä!
if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –Ω–∞ DoroLang
    sample_code = '''
# –≠—Ç–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
print "Hello, DoroLang!"
print 42
let name = "Dorofii"
print "My name is " + name
let age = 25
print age * 2
'''
    
    print("=== –ò–°–•–û–î–ù–´–ô –ö–û–î ===")
    print(sample_code)
    print("\n" + "="*50 + "\n")
    
    # –°–æ–∑–¥–∞–µ–º –ª–µ–∫—Å–µ—Ä –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
    lexer = Lexer(sample_code)
    tokens = lexer.tokenize()
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    lexer.print_tokens()
    
    print(f"\n–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {len(tokens)}")
    print("–õ–µ–∫—Å–µ—Ä –≥–æ—Ç–æ–≤! üéâ")